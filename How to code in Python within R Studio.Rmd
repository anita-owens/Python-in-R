---
title: "Python in R"
author: "Anita Owens"
output: html_notebook
---


## 1. Set up Environment

```{r Load R packages}
# Install pacman if needed
if (!require("pacman")) install.packages("pacman")

# load packages
pacman::p_load(pacman,
reticulate)
```

```{r Python Environment check}
#Check Anaconda environments
conda_list()
```
```{r Declare environment}
#Declare environment that you want to use
use_condaenv("anaconda3")
```

## 2. Install Packages

https://rstudio.github.io/reticulate/reference/py_install.html

```{r Install Packages}
#use py_install to install Python packages
py_install(c("pandas", "numpy", "matplotlib", "seaborn", "scikit-learn"))
```



```{python Working directory}
#import python os
from os import chdir, getcwd

#Change working directory of needed
wd=getcwd()
chdir(wd)

print(wd) #prints the current working directory
```



```{python Import python packages for data analysis and machine learning}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt 
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
```

## 3. Import data

```{python Import csv file}
#Import csv file as pandas DataFrame
credit = pd.read_csv("datasets/credit.csv")
```

```{python Check results of import}
#Check results of the import
print(credit.head(5))
```



```{python Check column data types and non-missing values}
# Check column data types and non-missing values
print(credit.info())
```
```{python Print just the column names}
print(credit.columns)
```



## 4. Data Exploration

```{python Data Visualization - part 1}
#Initialize subplots with number of rows and number of columns
figure, ax = plt.subplots(nrows = 2, ncols = 2)

#See the distribution of the data
sns.histplot(data=credit, x = 'Income', ax=ax[0,0])
sns.histplot(data=credit, x = 'Limit', ax=ax[0,1])
sns.histplot(data=credit, x = 'Rating',ax=ax[1,0])
sns.histplot(data=credit, x = 'Cards', ax=ax[1,1])

plt.tight_layout()
plt.show()
```

```{python Data Visualization - part 2}
#Initialize subplots with number of rows and number of columns
figure, ax = plt.subplots(nrows = 3, ncols = 2)

#See the distribution of the data
sns.histplot(data=credit, x = 'Age', ax=ax[0,0])
sns.histplot(data=credit, x = 'Education', ax=ax[0,1])
sns.countplot(data=credit, x = 'Gender',ax=ax[1,0])
sns.countplot(data=credit, x = 'Student', ax=ax[1,1])
sns.countplot(data=credit, x = 'Married', ax=ax[2,0])
sns.countplot(data=credit, x = 'Ethnicity', ax=ax[2,1])

plt.tight_layout()
plt.show()
```

```{python Data Visualization - part 3}
#Initialize subplots with number of rows and number of columns
figure, ax = plt.subplots(nrows = 2, ncols = 2)

#See the distribution of the data
sns.boxplot(data=credit, x = 'Gender', y = 'Balance', ax=ax[0,0])
sns.boxplot(data=credit, x = 'Student',y = 'Balance', ax=ax[0,1])
sns.boxplot(data=credit, x = 'Married', y = 'Balance',ax=ax[1,0])
sns.boxplot(data=credit, x = 'Ethnicity',y = 'Balance', ax=ax[1,1])

plt.tight_layout()
plt.show()
```
```{python Data Visualization - part 4}
sns.catplot(data=credit, x="Gender", y="Balance", hue="Student", kind="box")
plt.show()
plt.clf()

sns.catplot(data=credit, x="Married", y="Balance", hue="Student", kind="box")
plt.show()
plt.clf()

sns.catplot(data=credit, x="Ethnicity", y="Balance", hue="Student", kind="box")
plt.show()
plt.clf()

sns.lmplot(data = credit, x="Income", y="Balance", hue="Student")
plt.show()
```



```{python Data Visualization - part 5 - Pairplot}
plt.close('all')

#Plot pairwise relationships 
#We won't plot everything, just a few variables
sns.pairplot(credit, vars = ['Income', 'Limit', 'Rating', 'Cards'], hue='Student')

plt.show()
```
```{python Data Visualization - part 6 - Correlation plot}
#to find if there are relationships between variables.

plt.close('all')

matrix = credit.corr().round(2)
sns.heatmap(matrix, annot=True)
plt.show()
```


## 5. Data model - Linear Regression


```{python Prep data for modeling}
#Store categorical column names
categorical = credit.nunique()[credit.nunique() < 5].keys().tolist()

# Perform one-hot encoding on categorical variables only
credit_encoded = pd.get_dummies(data = credit, columns = categorical, drop_first=True)

# Store numerical column names
numerical = [x for x in credit.columns if x not in categorical]
```


```{python Scale numeric data}
# Initialize StandardScaler instance
scaler = StandardScaler()

# Fit and transform the scaler on numerical columns only
scaled_numerical = scaler.fit_transform(credit[numerical])

# Build a DataFrame from scaled_numerical object
scaled_numerical = pd.DataFrame(scaled_numerical, columns=numerical)
```

```{python Create final dataset}
#First, drop non-scaled numerical columns
credit_encoded  = credit_encoded.drop(columns = numerical, axis = 1)

#Next, merge the non-numerical with the scaled numerical data
credit_final = credit_encoded.merge(right=scaled_numerical,
                        how='left',
                        left_index=True, #similar to a LEFT JOIN 
                        right_index = True)
 
#Check results  - just column names               
print(credit_final.columns)                      
```



```{python Create X and Y DataFrames}
#All of our explanatory variables
X = credit_final[['Gender_Male', 'Student_Yes', 'Married_Yes', 'Ethnicity_Asian', 'Ethnicity_Caucasian', 'Income', 'Limit', 'Rating', 'Cards', 'Age', 'Education']].copy()

#Our response variable
Y = credit_final[['Balance']].copy()
```

Split data into test and train set

```{python Test Train split}
# Split X and Y into training and testing sets (80/20 split)
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20)
```



We want to predict credit card balance as a function of our independent variables.

```{python Model fit - Linear Regression}
#Create instance and call model fit
model = LinearRegression().fit(X_train,y_train)

#Make predictions
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)
```


```{python Goodness of fit measures for linear regression model}
#print scores
print("model.coef_: {}".format(model.coef_))
print("model.intercept_: {}".format(model.intercept_))
print('model train score %.3f, model test score: %.3f' % (
model.score(X_train,y_train),
model.score(X_test, y_test)))
```
Our regression model can explain about 95% percentage of variation or changes in balance.




